# Practical Time Series Analysis

### 실전 시계열 분석 | 통계와 머신러닝을 활용한 예측 기법

## ✔️ 시계열의 개요와 역사

---

🔍 **시계열 분석 :** 시간 순서대로 정렬된 데이터에서 의미 있는 요약과 통계 정보를 추출하기 위한 노력

- ‘과거가 미래에 어떤 영향을 주는가?’와 같은 인과관계를 다루는 질문과 그에 대한 대답
- 의학 ,기상학, 경제학, 천문학 등 다양한 학문 분야가 시계열 데이터를 분석하는 방법에 영향을 끼침
    - 시계열 문제로서의 의학 : 생명표(사망표) → 심전도(ECG) → 뇌전도(EEG) → 웨어러블 센서
    - 피츠로이의 일기예보, 리처드 데니스의 기계적 트레이닝(mechanical trading), 다양한 천문학 데이터
- 시계열 데이터는 여러 측면에서 높은 계산 능력을 요구하며, 오늘날 웨어러블 컴퓨터, 머신러닝, GPU 등이 데이터의 양과 품질을 혁신함
- 1920년 우드니 율이 처음으로 자기회귀 모델을 실제 데이터에 적용한 후 시계열 분석이 하나의 학문으로 시작함

## ✔️ 시계열 데이터의 발견과 다루기

---

💡 타임스탬프를 다루는 과정에서 가장 중요한 단계는 **데이터를 정리하고 제대로 처리하는 것**

⇒ 흩어진 열을 합치거나, 불규칙적이거나 유실된 데이터를 리샘플링하거나, 서로 다른 시간축으로 시계열을 정렬하는 등의 작업이 필요

💡 **시계열 데이터 수집 목적**

1. 학습과 실험 목적에 맞는 데이터셋 찾기
    - 캐글과 같은 대회용 데이터셋이나 저장소 데이터셋과 같이 미리 준비된 데이터셋
    - UCI 머신러닝 저장소, UEA 및 UCR 시계열 분류 저장소, 정부 시계열 데이터셋, CompEngine, R 패키지 등등
2. 시간 지향적인 형태가 아닌 데이터에서 시계열 데이터 생성하기
    - **발견된 시계열**(found time series) : 야생에서 직접 수집한 시계열 데이터 (타임스탬프는 어디에나 존재할 수 있다는 관점)
    - 파일에 접근한 시간 기록만으로도 시간의 변화량을 모델링하고 더 큰 기간에 대한 통계적인 분석이 가능
    - 명시적으로 시간을 포함하지 않지만, 데이터셋의 숨은 논리로 시간이 설명되는 경우 변수 중 하나를 시간에 대응할 수 있음
    - 많은 과학 학문 분야에서 디지털 형태로 저장되는 물리적인 흔적들도 시계열 데이터로 활용될 수 있음

🔍 **사전관찰**(Lookahead) : 데이터를 통해 실제로 알아야 하는 시점보다 더 일찍 미래에 대한 사실을 발견하는 방법

- 어떻게든 미래에 일어날 일에 대한 정보가 모델에서 시간을 거슬러 전파되어 모델의 초기 동작에 영향을 주는 방법

💡 **테이블 집합에서 시계열 데이터 집합 개선하기**

1. 해결하고자 하는 문제에 맞는 형태로 **데이터의 간격**을 교정할 것
    
    → 데이터가 필요 이상으로 구체적인 시간 정보를 제공할 경우, 적절한 주기의 데이터로 변환해야 함
    
2. **사전관찰을 피하기 위해** 가용 데이터를 생산하는 타임스탬프를 데이터에 사용하지 않도록 주의할 것
3. ‘아무 일도 일어나지 않았더라도’ **관련된 모든 기간**을 기록할 것
    
    → 해당 기간의 합계가 0이라고 하더라도 충분히 유익한 정보가 될 수 있음
    
4. **사전관찰을 피하기 위해** 아직 알아서는 안 되는 정보를 생산하는 타임스탬프를 데이터에 사용하지 않도록 주의할 것

💡 **타임스탬프의 문제점**

1. 무엇에 대한 타임스탬프인가? - 생성 과정, 방법, 시기에 대한 질문 던지기
    - 기록된 타임스탬프는 사용자가 작업을 시작한 시점, 앱이 기록한 시점, 메타데이터가 업로드된 시점 등등 여러 의미를 가질 수 있음
    - 따라서, 타임스탬프를 발견하면 가장 먼저 **“그 사건의 시간이 무엇에 의해 정해졌는지”**를 최대한 파악하는 것이 중요함
2. 타임스탬프를 추측하여 데이터 이해하기
    - 작성자 및 관리자와 직접 논의하는 것이 가장 좋지만, 현실적으로 불가능할 경우 자체적으로 타임스탬프의 의미를 추론해야 함
    - 현지 시간으로 데이터를 저장하는 일은 매우 드물지만, 어떤 경우는 야생에서 발견될 수 있기 때문에 모든 가능성을 고려해야 함
        - 타임스탬프가 현지의 시간에 기반한다는 가설을 세웠다면, 낮과 밤 시간을 반영한 데이터의 일일 추세를 확인해야 함
        - 하루 위주의 패턴을 발견하지 못한다면 해당 데이터의 타임스탬프는 협정 세계시간에 기반하였다고 볼 수 있음
    - 타임스탬프가 사용자의 행동에 대한 것인지 아니면 네트워크의 동작에 대한 것인지 파악할 필요가 있음
        - 사용자별 타임스탬프 간의 차이로 데이터 항목 간의 간격에 대한 감을 얻을 수 있음
        - 사용자의 행동을 종합적으로 묘사해 서버가 활성될 가능성이 높은 시점을 24시간 주기로 판단할 수 있음
3. 의미 있는 시간 규모란
    - 적당히 가감하여 기록하는 타임스탬프의 단위는 대상 도메인에 대한 지식과 데이터의 상세한 수집 방식에 근거하여 다뤄져야 함
    

💡 **시계열에서 누락된 데이터를 해결하는 방법**

1. **대치법**(imputation) : 데이터셋 전체의 관측에 기반하여 누락된 데이터를 채워 넣는 방법
    1. 포워드 필(forward fill) : 누락된 값이 나타나기 직전의 값으로 누락된 값을 채우는 방법
        - 이미 기록된 데이터 외에 참고할 수 있는 정보가 없는 경우 가장 최근의 측정값을 사용하는 것이 타당함
        - 계산이 복잡하지 않고, 실시간 스트리밍 데이터에 쉽게 적용할 수 있으며, 대치 작업을 능숙하게 처리함
    2. 백워드 필(backward fill) : 누락된 값이 나타난 직후의 값으로 누락된 값을 채우는 방법
        - 이는 사전관찰이기 때문에, 미래를 예측하지 않거나 데이터의 미래보다 과거를 채우는게 의미 있는 경우에만 사용해야 함
    3. 이동평균(moving average, 롤링평균)
        - 과거의 값으로 미래의 값을 ‘예측’한다는 점은 포워드 필과 같으나, 최근 과거의 여러 시간대를 사용한다는 점에서 다름
        - 전체 평균에 관한 개별 데이터값을 의심할 만한 이유가 있는 경우, 포워드 필보다 더 타당함 (임의의 노이즈 일부를 제거할 수 있음)
            - 지수가중이동평균은 비교적 최근 시점의 데이터에 더 많은 가중치를 주어야 하는 경우에 유용
            - 기하평균은 일련의 데이터가 강한 상관관계를 가지고 시간이 지나면서 복합적인 값을 가지는 경우에 유용
        - 사전관찰이 포함되어서는 안 되는 경우에는 누락이 발생되기 이전의 데이터만 사용해야 하지만,
        그렇지 않은 경우에는 누락된 데이터의 전과 후의 값을 포함해야 추정에 사용되는 정보를 최대화할 수 있음
        - 이동평균에 의한 대치는 데이터셋의 분산 정도를 줄여주며, 정확도, R2 통계량 등 모델 성능 지표 계산 시 과대평가의 가능성이 낮아짐
        
        <aside>
        ⚠️ 특정 시점만 다루는 경우에는 전체 평균이나 중앙값을 사용하지만, 시계열에서 이는 **미래를 들여다 보는 행위**이며 곧 사전관찰을 의미함
        
        </aside>
        
2. **보간법**(interpolation) : 대치법의 한 형태로 인접한 데이터를 사용하여 누락된 데이터를 추정하는 방법 (기하학적인 행동에 제한하여 결정)
    1. 선형 보간법(linear interpolation) : 누락된 데이터가 주변 데이터에 **선형적인 일관성**을 갖도록 제한하는 방법
        - 시간에 따라 시스템이 동작하는 방식을 이미 알고 있고, 그 지식을 활용할 때 특히 유용 (베이즈 관점에서의 선험성)
        - 과거와 미래의 데이터를 모두 활용하거나 둘 중 하나만 활용할 수 있음 (사전관찰을 고려하여 선택)
3. 영향받은 기간 **삭제** : 누락된 데이터의 기간을 완전히 사용하지 않는 방법

💡 **시계열의 샘플링 빈도 변경하는 방법**

⇒ 데이터의 출처가 서로 달라 샘플링 빈도가 같지 않은 경우 적절한 방식으로 데이터의 빈도를 변경해야 함

1. 다운샘플링(Downsampling) : 원본 시계열보다 타임스탬프가 **더 낮은 빈도로 발생하게끔** 데이터의 부분집합을 만드는 것
    1. 원본 데이터의 시간 단위가 실용적이지 않은 경우
        - 지나치게 자주 측정한 경우, 데이터의 저장 공간 및 처리에 대한 부담을 감수할 만큼 새로운 정보를 제공하지 않음
        - 규칙적으로 샘플링된 데이터의 경우, 다운샘플링은 매 n개의 요소를 추출하는 것만큼 작업이 간단함
    2. 계절 주기의 특정 부분에 집중하는 경우
        - 시계열의 전반적인 계절성보다 하나의 특정 계절에만 초점을 맞추어야 하는 경우 연간 빈도로 다운샘플링해야 함
    3. 더 낮은 빈도의 데이터에 맞추는 경우
        - 한 데이터를 낮은 빈도로 측정된 다른 데이터와 맞춰주기 위해 데이터를 취합하거나 다운샘플링해야 함
        - 단순히 평균, 합계 등을 구할 수도 있지만, 나중의 값에 가중치를 더 부여하는 등 더 복잡한 과정을 요하기도 함
2. 업샘플링(Upsampling) : 데이터가 실제보다 **더 자주 수집된 것처럼** 표현하는 것
    - 실제로 측정하는 것은 아니지만 드물게 측정된 데이터에서 더 조밀한 시간의 데이터를 얻고자 하는 과정
        
        > 시계열을 낮은 주기에서 높은 주기로 바꿀 수는 없습니다.
        예를 들어 데이터를 주간에서 일간으로, 일간에서 5분 단위로 변환하기 위해서는 마법과도 같은 능력이 필요합니다.
        > 
        
        ⇒ 더 많은 시간의 레이블을 추가하는 것일 뿐, 더 많은 정보 자체를 추가할 수는 없다는 한계점이 있음
        
    1. 불규칙적으로 샘플링된 시계열을 규칙적인 형태로 변환하는 경우
        - 데이터 사이의 시차보다 더 높은 빈도로 전체 데이터를 변환해야 할 가능성이 높음
    2. 입력이 서로 다른 빈도로 샘플링되었고 더 높은 빈도의 데이터에 맞춰야 하는 경우
        - 지금까지 알려진 상태로만 추측하는 경우라면 사전관찰 없이 안전한 업샘플링이 가능함
        - 업샘플링을 데이터 누락에 대한 문제로 간주하여 보간법 등으로 데이터를 채워넣을 수도 있음

💡 **데이터 평활** - 지수평활(exponential smoothing)

- 이동평균과는 대조적으로, 좀 더 최근 데이터일수록 더 많은 가중치를 줘서 시간의 특성을 더 잘 인식하도록 함
- 특정 윈도에 포함된 데이터가 시간상 가까우면 가장 많은 가중치를, 과거의 데이터일수록 기하급수적으로 낮은 가중치를 적용함
- 지수평활 공식
    - t 시점에서 평활된 값 : $S_t = d * S_{t-1} + (1-d) * x_t$
    - t-1 시점에서 평활된 값 : $S_{t-1} = d * S_{t-2} + (1-d) * x_{t-1}$
    
     ⇒ 결과적으로 연쇄적 형태로 계산되어짐 ( $d^3 * x_{t-3} + d^2 * x_{t-2} + d * x_{t-1}$)
    
- 평활요인으로도 불리는 `alpha` 파라미터 값이 크면 클수록 있는 그대로의 현재 값에 가깝도록 더 빨리 갱신됨

🔍 **계절성 데이터**와 **순환성 데이터**

- 계절성 시계열 : 일련의 동작이 정해진 기간 동안 반복되는 데이터로, 여러 개의 주기성이 있을 수 있음 → 사람의 행동과 관련된 대부분의 시계열
- 순환성 시계열 : 계절성 시계열과 같이 반복적인 동작을 보이나, 그 기간이 가변적임 → 주식시장의 호황과 불황의 주기, 화산 폭발 주기 등등
- 데이터를 **계절성**(seasonal), **추세**(trend), **나머지**(remainder) 요소로 분해해서 비교하기 (R 코드 : `plot(stl(AirPassengers, "periodic"))`)

## ✔️ 시계열의 탐색적 자료 분석

---

💡 **일반적인 EDA**

1. 도표 그리기
    - 단순히 `plot()` 함수만으로도 데이터를 서로 다른 시계열의 그래프로 자동 분할할 수 있음
    - ts와 ts로부터 파생된 클래스 활용 : `frequency()` - 데이터의 연간 빈도, `start()` - 시작 시점, `end()` - 끝 시점, `window()` - 시간의 한 부분 범위
2. 히스토그램
    - 시계열 데이터에서는 실제 측정치보다 한 측정치가 다음 측정치로 변화한 정도를 더 눈여겨보아야 함
    - **시간상 인접한 데이터의 차이**(difference; 차분)에 대한 히스토그램이 보다 효과적으로 시계열 데이터를 설명함
3. 산점도
    - 특정 시점에서의 컬럼 간의 상관관계 / 시간에 따른 각 컬럼의 변화량 간의 상관관계 (R의 `diff()` 함수)
    - 시간상 먼저 알게 된 한 컬럼의 변동으로 나중의 다른 컬럼의 변동을 예측하는 것이 보다 명확함 (R의 `lag()` 함수)
    
    ⇒ 비시계열 데이터와 동일한 탐색 기법을 사용할 수는 있으나, 시계열의 특성에 대한 이해가 없으면 유의미한 결과를 낼 수 없음
    
    ⇒ 시간에 따른 변화나 서로 다른 시점의 데이터 사이의 관계가 데이터의 동작 방식을 이해하는 데에 유익한 정보인 경우가 많음
    

💡 **시계열에 특화된 EDA** - 정상성, 자기상관, 허위상관을 고려해야 함

1. 정상성(stationarity) : 시계열이 시스템의 안정성(stable)을 반영하는가 아니면 지속적인 변화를 반영하는가
    - 정상이 아닌 데이터의 특징 : 평균값이 시간에 따라 증가 혹은 감소함 / 분산값이 시간에 따라 증가 혹은 감소함 / 계절성 혹은 순환성을 가짐
    - 확대된 디키-풀러(ADF) 검정 : 시계열의 정상성 문제를 가장 보편적으로 평가하는 평가 지표
    - 평균을 정상화하기 위해서는 **차분**을, 분산을 정상화하기 위해서는 **로그/제곱근 변환**을 활용하는 것이 일반적임
    - 윈도 함수(ex. 롤링평균)를 통해 시계열 데이터의 추세를 확인하거나 데이터의 편차가 어떤 의미를 가지는지를 파악할 수 있음
2. 자기상관(autocorrelation) : 시계열 내부에서 미래 예측에 먼 과거 또는 최근의 데이터가 얼마나 밀접한 연관성을 가지는가
    - 자기상관함수, 부분자기상관함수 등으로 시계열 데이터가 시간에 의존하지 않고 무작위성을 띠는지의 여부를 알 수 있음
3. 허위상관(spurious correlation) : 시계열 내부에서 발견된 특정 행동역학이 모두 실제 인과관계로 이어지는 것은 아니다

## ✔️ 시계열 특징의 생성 및 선택

---

💡 시계열을 위한 특징 생성 : 시계열 데이터의 가장 중요한 특성을 정량화하여 수치 및 범주형 레이블로 압축하는 방법을 찾는 과정

- 전체 시계열에 대해 가능한 한 많은 정보를 적은 수의 지표로 압축하고, 이를 통해 시계열의 가장 중요한 정보를 식별하기 위함
- 분석의 타당성을 훼손하지 않는지, 유의미한 통찰을 이끌어낼 수 있는지, 과하게 생성한 특징이 과적합을 이끌어내지 않는지 고려해야 함
- EDA를 통해 데이터의 예측 및 분류에 유용한 특징의 종류를 발굴하거나 도메인에 대한 배경지식을 활용하여 가설을 세우는 것이 바람직함
- 시계열 특징 생성 시 고려 사항
    - 시계열의 기본 성질(정상성, 시계열의 길이 등), 도메인 지식, 계산 및 스토리지 리소스의 정도 등
- 일반적으로 사용되는 요약 통계 특징
    - 평균, 중앙값 등의 중심 경향성, 최대값, 최소값, 범위, 분산, 표준편차 등의 산포도
    - 국소적 범위에서의 최소 및 최대 개수, 시계열의 평활 정도, 주기성 및 자기상관 정도

💡 시계열을 위한 특징 선택

- **FRESH**(FeatuRe Extraction based on Scalable Hypothesis test; 확장 가능한 가설 검정에 기반한 특징 추출)
    
    : 타깃 변수에 대한 각 입력 특징의 p값을 계산하여 그 중요도를 평가 (Python의 `tsfresh` 모듈 사용)
    
    ⇒ 특징 간의 의존성을 설명하는 데에 유용하나, 그 설명의 이유를 추론하기는 어려움
    
- **RFE**(Recursive Feature Elimination; 재귀특징제거법) : 포괄적인 모델에서 점차 특징을 제거하면서 점진적으로 특징을 선택 (Python의 `RFE` 모듈 사용)
    
    ⇒ 특징 선택과 중요성의 순위를 매기는 방법 모두에 사용할 수 있음
    

💡 시계열 특징 생성 및 선택의 유용성

- 대다수의 머신러닝 알고리즘은 시계열 그 자체보다는 특징들의 집합으로 표현된 데이터를 수용하게끔 설계되어있음
    
    ⇒ 머신러닝 알고리즘에 활용될 수 있는 형식으로 시계열의 데이터를 구성할 때 유용
    
- 전체적인 시계열 데이터의 관측 정보를 적은 수의 정량적 지표로 압축하여 요약하는 데에 유용
- 다양한 조건에서 측정되어 비교하기 어려운 데이터를 보다 광범위하게 요약함으로써 쉽게 비교할 수 있음

## ✔️ 시계열을 위한 머신러닝

---

💡 **의사결정 트리** 기반 머신러닝 : 한 번에 한 단계씩, 비선형적인 방식으로 결정을 내리는 방식

1. 랜덤포레스트(random forest)
    - 여러 개의 결정 트리를 사용하여 각 트리가 내놓은 출력의 평균으로 분류 및 예측 수행 (배깅 기반)
    - 학습될 트리의 개수, 각 트리에 허용되는 최대 깊이에 대한 하이퍼파라미터에 따라 그 구조가 결정됨
    - 개별 모델의 성능이 좋지 않더라도 모두 모으면 충분히 일반화된 결과를 도출해낼 수 있음
    - 원시 시계열 데이터보다 특징들로 압축된 요약 데이터를 사용할 때 훨씬 유용한 알고리즘 (예측보다 분류에서 더 효과적)
2. 그레이디언트 부스팅(boosting)
    - 차례로 생성되는 모델 중 이전 모델에서 잘못 적합된 데이터가 나중 모델에서 더 큰 가중치를 부여받음(부스팅 기반)
    - 모델의 복잡도에 대한 패널티 항을 포함한 손실 함수를 최소화하며, 이 패널티 항은 생성될 트리의 개수를 제한함
    - 랜덤포레스트와 대조적으로, 시계열 예측과 분류 모두에서 아주 효과적인 알고리즘
    - 랜덤포레스트보다 실행 속도 면에서 향상된 모델로, 대규모의 데이터셋에서 보다 효과적인 알고리즘
    
    ⇒ 배깅은 덜 의미 있는 특징도 사용하도록 강요할 수 있지만, 부스팅은 관련이 큰 것에 특혜를 주므로 의미 없는 특징이 무시될 가능성이 높음

## ✔️ 시계열을 위한 딥러닝

---

💡 




## Reference :

- PacktPublishing / Practical-Time-Series-Analysis
 : https://github.com/PacktPublishing/Practical-Time-Series-Analysis

- 실전 시계열 분석 리뷰.pdf / 
 : https://s3.us-west-2.amazonaws.com/secure.notion-static.com/24ee898a-a40b-4701-91a1-2c1b33adfa8a/%EC%8B%A4%EC%A0%84_%EC%8B%9C%EA%B3%84%EC%97%B4_%EB%B6%84%EC%84%9D_%EB%A6%AC%EB%B7%B0.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220705%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220705T034813Z&X-Amz-Expires=86400&X-Amz-Signature=feb527a3136967d90024eaedec0de8d8a0dbb56dadc64b53c42a98d8fc13a73c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22%25EC%258B%25A4%25EC%25A0%2584%2520%25EC%258B%259C%25EA%25B3%2584%25EC%2597%25B4%2520%25EB%25B6%2584%25EC%2584%259D%2520%25EB%25A6%25AC%25EB%25B7%25B0.pdf%22&x-id=GetObject